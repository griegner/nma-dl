{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download train + test datasets\n",
    "train_dataset = {\"path\": \"../datasets/train-dataset\", \n",
    "                 \"url\": \"http://data.celltrackingchallenge.net/training-datasets/Fluo-N2DH-GOWT1.zip\"}\n",
    "test_dataset = {\"path\": \"../datasets/test-dataset\",\n",
    "                \"url\": \"http://data.celltrackingchallenge.net/test-datasets/Fluo-N2DH-GOWT1.zip\"}\n",
    "\n",
    "# shell commands\n",
    "!mkdir -p ../datasets\n",
    "!wget -nc -O {train_dataset[\"path\"]}.zip {train_dataset[\"url\"]}\n",
    "!wget -nc -O {test_dataset[\"path\"]}.zip {test_dataset[\"url\"]}\n",
    "!unzip -n {train_dataset[\"path\"]}.zip -d {train_dataset[\"path\"]}\n",
    "!unzip -n {test_dataset[\"path\"]}.zip -d {test_dataset[\"path\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device:\\t{DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tif images as np.uint8\n",
    "X_train = plt.imread(f\"{train_dataset['path']}/Fluo-N2DH-GOWT1/01/t000.tif\")\n",
    "y_train = plt.imread(f\"{train_dataset['path']}/Fluo-N2DH-GOWT1/01_ST/SEG/man_seg000.tif\")\n",
    "\n",
    "# plot and example train-test pair\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.tight_layout()\n",
    "axs[0].imshow(X_train, cmap=\"binary\")\n",
    "axs[0].set_title(\"X_train\")\n",
    "axs[1].imshow(y_train, cmap=\"CMRmap_r\")\n",
    "axs[1].set_title(\"y_train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(data.Dataset):\n",
    "    def __init__(self, X_path, y_path, preprocess_transforms=None, augmentation_transforms=None):\n",
    "        \"\"\"Create a segmentation dataset for pytorch dataloader.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_path : str\n",
    "            Path to the directory with raw images.\n",
    "        y_path : str\n",
    "            Path to the directory with labeled images.\n",
    "        preprocess_transforms : torchvision.transforms.Compose, optional\n",
    "            Transforms to be applies on train/validation/test images, by default None\n",
    "        augmentation_transforms : torchvision.transforms.Compose, optional\n",
    "            Transforms to be applied on train images only, by default None\n",
    "        \"\"\"\n",
    "        self.X_path = X_path\n",
    "        self.y_path = y_path\n",
    "        self.preprocess_transforms = preprocess_transforms\n",
    "        self.augmentation_transforms = augmentation_transforms\n",
    "        self.X = sorted(Path(self.X_path).glob(\"*.tif\"))\n",
    "        self.y = sorted(Path(self.y_path).glob(\"*.tif\"))\n",
    "        assert len(self.X) == len(self.y), \"len(X) != len(y)\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"number of training images\"\"\"\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"load X-y image pair and apply transformations.\"\"\"\n",
    "\n",
    "        # load images as tensors\n",
    "        X_img = plt.imread(self.X[idx]).astype(np.uint8)\n",
    "        y_img = plt.imread(self.y[idx]).astype(np.uint8)\n",
    "\n",
    "        # apply image transformations\n",
    "        if self.preprocess_transforms:\n",
    "            X_img = self.preprocess_transforms(X_img)\n",
    "            y_img = self.preprocess_transforms(y_img)\n",
    "\n",
    "        if self.augmentation_transforms:\n",
    "            X_img = self.augmentation_transforms(X_img)\n",
    "            y_img = self.augmentation_transforms(y_img)\n",
    "\n",
    "        y_img[y_img != 0] = 1  # binarize\n",
    "\n",
    "        return X_img, y_img\n",
    "\n",
    "\n",
    "def get_data_loaders(X_path, y_path, batch_size=20, seed=2023, augmentation_transforms=True):\n",
    "    \"\"\"Helper function to initialize pytorch dataloaders.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_path : str\n",
    "        Path to the directory with raw images.\n",
    "    y_path : str\n",
    "        Path to the directory with labeled images.\n",
    "    batch_size : int, optional\n",
    "        Number of samples / batch to load, by default 20\n",
    "    seed : int, optional\n",
    "        Random seed, by default 2023\n",
    "    augmentation_transforms : bool, optional\n",
    "        Whether to apply augmentation transformations to training sets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        Dataloader for the training set.\n",
    "    val_loader : torch_utils.data.DataLoader\n",
    "        Dataloader for the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    # random number generator\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # transformations to train/validation/test images\n",
    "    preprocess_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Resize([572, 572], antialias=True)]\n",
    "    )  # for input to U-Net\n",
    "\n",
    "    # transformations to train images\n",
    "    if augmentation_transforms:\n",
    "        augmentation_transforms = transforms.Compose(\n",
    "            [transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip()]\n",
    "        )\n",
    "    else:\n",
    "        augmentation_transforms = None\n",
    "\n",
    "    # train-validation split\n",
    "    segmentation_dataset = SegmentationDataset(\n",
    "        X_path, y_path, preprocess_transforms, augmentation_transforms\n",
    "    )\n",
    "    train_dataset, val_dataset = data.random_split(segmentation_dataset, [0.8, 0.2], generator=rng)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = data.DataLoader(val_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_data_loaders(X_path, y_path):\n",
    "    \"\"\"Test that data loaders are working as expected\"\"\"\n",
    "    batch_size = 37\n",
    "    train_loader, val_loader = get_data_loaders(X_path, y_path, batch_size)\n",
    "\n",
    "    assert len(train_loader.dataset) > 0, \"training set is empty\"\n",
    "    assert len(val_loader.dataset) > 0, \"validation set is empty\"\n",
    "    assert len(train_loader.dataset) / len(train_loader) == batch_size, \"batch_size error\"\n",
    "\n",
    "    X, y = next(iter(train_loader))\n",
    "    assert X.shape == y.shape, \"training set error\"\n",
    "\n",
    "    X, y = next(iter(val_loader))\n",
    "    assert X.shape == y.shape, \"validation set error\"\n",
    "\n",
    "\n",
    "test_get_data_loaders(\n",
    "    f\"{train_dataset['path']}/Fluo-N2DH-GOWT1/01\",\n",
    "    f\"{train_dataset['path']}/Fluo-N2DH-GOWT1/01_ST/SEG\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net: Convolutional Networks for Biomedical Image Segmentation (https://arxiv.org/abs/1505.04597).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn : torch.nn.Module\n",
    "        Neural network module to use as the base architecture for the U-Net.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The U-Net model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Encoder\n",
    "        # -------\n",
    "        self.maxpool2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.downconv1 = self.double_conv(1, 64)\n",
    "        self.downconv2 = self.double_conv(64, 128)\n",
    "        self.downconv3 = self.double_conv(128, 256)\n",
    "        self.downconv4 = self.double_conv(256, 512)\n",
    "        self.downconv5 = self.double_conv(512, 1024)\n",
    "\n",
    "        # Decoder\n",
    "        # -------\n",
    "        self.uptrans1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.upconv1 = self.double_conv(1024, 512)\n",
    "        self.uptrans2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.upconv2 = self.double_conv(512, 256)\n",
    "        self.uptrans3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.upconv3 = self.double_conv(256, 128)\n",
    "        self.uptrans4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.upconv4 = self.double_conv(128, 64)\n",
    "\n",
    "        # Output Layer\n",
    "        # ------------\n",
    "        self.output = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"Forward pass of the U-Net model.\"\"\"\n",
    "\n",
    "        # Encoder\n",
    "        # -------\n",
    "        self.e1 = self.downconv1(img)  # ---> e1\n",
    "        self.e2 = self.maxpool2x2(self.e1)\n",
    "        self.e3 = self.downconv2(self.e2)  # ---> e3\n",
    "        self.e4 = self.maxpool2x2(self.e3)\n",
    "        self.e5 = self.downconv3(self.e4)  # ---> e5\n",
    "        self.e6 = self.maxpool2x2(self.e5)\n",
    "        self.e7 = self.downconv4(self.e6)  # ---> e7\n",
    "        self.e8 = self.maxpool2x2(self.e7)\n",
    "        self.e9 = self.downconv5(self.e8)\n",
    "\n",
    "        # Decoder\n",
    "        # -------\n",
    "        self.d1 = self.uptrans1(self.e9)\n",
    "        self.e7_crop = self.crop_img(self.e7, self.d1)  # <--- e7\n",
    "        self.d2 = self.upconv1(torch.cat([self.d1, self.e7_crop], 1))\n",
    "        self.d3 = self.uptrans2(self.d2)\n",
    "        self.e5_crop = self.crop_img(self.e5, self.d3)  # <--- e5\n",
    "        self.d4 = self.upconv2(torch.cat([self.d3, self.e5_crop], 1))\n",
    "        self.d5 = self.uptrans3(self.d4)\n",
    "        self.e3_crop = self.crop_img(self.e3, self.d5)  # <--- e3\n",
    "        self.d6 = self.upconv3(torch.cat([self.d5, self.e3_crop], 1))\n",
    "        self.d7 = self.uptrans4(self.d6)\n",
    "        self.e1_crop = self.crop_img(self.e1, self.d7)  # <--- e1\n",
    "        self.d8 = self.upconv4(torch.cat([self.d7, self.e1_crop], 1))\n",
    "\n",
    "        # Output Layer\n",
    "        # ------------\n",
    "        self.d9 = self.output(self.d8)\n",
    "        # !! edit from U-Net paper: matching the original image size!!\n",
    "        return nn.functional.interpolate(self.d9, (572, 572), mode=\"nearest\")\n",
    "\n",
    "    def double_conv(self, input_channels, output_channels):\n",
    "        \"\"\"Double convolution with ReLU activation.\"\"\"\n",
    "        conv2d = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channels, output_channels, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        return conv2d\n",
    "\n",
    "    def crop_img(self, input_img, output_img):\n",
    "        \"\"\"Crop the input image to match the size (width, hight) of the output image.\"\"\"\n",
    "        input_size = input_img.shape[2]\n",
    "        output_size = output_img.shape[2]\n",
    "        delta_size = (input_size - output_size) // 2\n",
    "        return input_img[\n",
    "            :, :, delta_size : input_size - delta_size, delta_size : input_size - delta_size\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unet_shape():\n",
    "    \"\"\"Test expected dimensions for each layer of the U-Net\"\"\"\n",
    "    img = torch.rand((1, 1, 572, 572))\n",
    "    n_classes = 3\n",
    "    unet = UNet(n_classes=n_classes)\n",
    "    output = unet(img)\n",
    "\n",
    "    # Encoder\n",
    "    # -------\n",
    "    assert unet.e1.shape == (1, 64, 568, 568), \"e1\"\n",
    "    assert unet.e2.shape == (1, 64, 284, 284), \"e2\"\n",
    "    assert unet.e3.shape == (1, 128, 280, 280), \"e3\"\n",
    "    assert unet.e4.shape == (1, 128, 140, 140), \"e4\"\n",
    "    assert unet.e5.shape == (1, 256, 136, 136), \"e5\"\n",
    "    assert unet.e6.shape == (1, 256, 68, 68), \"e6\"\n",
    "    assert unet.e7.shape == (1, 512, 64, 64), \"e7\"\n",
    "    assert unet.e8.shape == (1, 512, 32, 32), \"e8\"\n",
    "    assert unet.e9.shape == (1, 1024, 28, 28), \"e9\"\n",
    "\n",
    "    # Decoder\n",
    "    # -------\n",
    "    assert unet.d1.shape == (1, 512, 56, 56), \"d1\"\n",
    "    assert unet.e7_crop.shape == (1, 512, 56, 56), \"e7_crop\"\n",
    "    assert unet.d2.shape == (1, 512, 52, 52), \"d2\"\n",
    "    assert unet.d3.shape == (1, 256, 104, 104), \"d3\"\n",
    "    assert unet.e5_crop.shape == (1, 256, 104, 104), \"e5_crop\"\n",
    "    assert unet.d4.shape == (1, 256, 100, 100), \"d4\"\n",
    "    assert unet.d5.shape == (1, 128, 200, 200), \"d5\"\n",
    "    assert unet.e3_crop.shape == (1, 128, 200, 200), \"e3_crop\"\n",
    "    assert unet.d6.shape == (1, 128, 196, 196), \"d6\"\n",
    "    assert unet.d7.shape == (1, 64, 392, 392), \"d7\"\n",
    "    assert unet.e1_crop.shape == (1, 64, 392, 392), \"e1_crop\"\n",
    "    assert unet.d8.shape == (1, 64, 388, 388), \"d8\"\n",
    "\n",
    "    # Output Layer\n",
    "    # ------------\n",
    "    assert unet.d9.shape == (1, n_classes, 388, 388), \"d9\"\n",
    "    assert output.shape == (1, n_classes, 572, 572), \"output\"\n",
    "\n",
    "\n",
    "test_unet_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(unet, train_loader, optimizer, loss_function, device=\"cuda\"):\n",
    "    \"\"\"Train a U-Net model for one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unet : torch.nn.Module\n",
    "        The U-Net model to train.\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        The data loader for the training dataset.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_function : callable\n",
    "        The loss function to use for parameter optimization.\n",
    "    device : str, optional\n",
    "        The device to use for training. Default is \"cuda\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The trained U-Net model.\n",
    "    \"\"\"\n",
    "    unet.train()  # set model to training mode (retains gradients)\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)  # mv to cpu or cuda\n",
    "        optimizer.zero_grad()  # clear gradients btw batches\n",
    "        # !! dimension change: (bs, 1, 572, 572) > (bs, 2, 572, 572) !!\n",
    "        y_hat = unet(X)  # forward pass\n",
    "        loss = loss_function(y_hat[:, :1, :, :], y)\n",
    "        loss.backward()  # backward pass\n",
    "        optimizer.step()  # update parameters\n",
    "\n",
    "    return unet\n",
    "\n",
    "\n",
    "def test_unet(unet, test_loader, loss_function, device=\"cuda\"):\n",
    "    \"\"\"Calculate validation/test accuracy for the U-Net model against labeled images.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unet : torch.nn.Module\n",
    "        Trained U-Net model.\n",
    "    test_loader : torch.utils.data.DataLoader\n",
    "        The data loader for the validation/test set.\n",
    "    loss_function : callable\n",
    "        The loss function to use for parameter optimization.\n",
    "    device : str, optional\n",
    "        The device to use for testing. Default is \"cuda\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average loss across the validation/test sets.\n",
    "    \"\"\"\n",
    "    unet.eval()  # set model to evaluation mode (gradients not retained)\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)  # mv to cpu or cuda\n",
    "            y_hat = unet(X)\n",
    "            # sum of batch loss\n",
    "            batch_loss += loss_function(y_hat[:, :1, :, :], y, reduction=\"sum\").item()\n",
    "\n",
    "    batch_loss /= len(test_loader.dataset)  # average loss across batches\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "def main(args, unet, train_loader, val_loader, device=\"cuda\"):\n",
    "    \"\"\"Train and validate U-Net model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : dict {n_epochs: <int>, learning_rate: <float>}\n",
    "        A dictionary specifying number of epochs and learning rate.\n",
    "    unet : torch.nn.Module\n",
    "        The U-Net model to be trained and validated.\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        The data loader for the training set.\n",
    "    val_loader : torch.utils.data.DataLoader\n",
    "        The data loader for the validation set.\n",
    "    device : str, optional\n",
    "        The device to use for training and validation, by default \"cuda\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the trained model and the validation loss history.\n",
    "    \"\"\"\n",
    "    unet = unet.to(device)  # mv model to \"cpu\" or \"cuda\"\n",
    "    optimizer = optim.Adam(unet.parameters(), lr=args[\"learning_rate\"])\n",
    "    loss_function = nn.BCEWithLogitsLoss()  # binary cross-entropy loss\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    for _ in tqdm(range(args[\"n_epochs\"])):\n",
    "        unet = train_unet(unet, train_loader, optimizer, loss_function, device)\n",
    "        train_loss = test_unet(unet, train_loader, loss_function, device)\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss = test_unet(unet, val_loader, loss_function, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! train model !!\n",
    "Xy_paths = (\n",
    "    f\"{train_dataset['path']}/Fluo-N2DH-GOWT1/01\",\n",
    "    f\"{train_dataset['path']}/Fluo-N2DH-GOWT1/01_ST/SEG\",\n",
    ")\n",
    "train_loader, val_loader = get_data_loaders(Xy_paths[0], Xy_paths[1], batch_size=10)\n",
    "n_classes = 2\n",
    "args = dict(n_epochs=1, learning_rate=1e-4)\n",
    "unet = UNet(n_classes)\n",
    "train_losses, val_losses = main(args, unet, train_loader, val_loader, device=DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nma-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
